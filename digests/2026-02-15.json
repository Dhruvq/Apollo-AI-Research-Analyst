{
  "cycle_id": "2026-02-15",
  "generated": "2026-02-23",
  "paper_count": 25,
  "papers": [
    {
      "id": "2602.15513v1",
      "title": "Improving MLLMs in Embodied Exploration and Question Answering with Human-Inspired Memory Modeling",
      "abstract": "Deploying Multimodal Large Language Models as the brain of embodied agents remains challenging, particularly under long-horizon observations and limited context budgets. Existing memory assisted methods often rely on textual summaries, which discard rich visual and spatial details and remain brittle in non-stationary environments. In this work, we propose a non-parametric memory framework that explicitly disentangles episodic and semantic memory for embodied exploration and question answering. Our retrieval-first, reasoning-assisted paradigm recalls episodic experiences via semantic similarity and verifies them through visual reasoning, enabling robust reuse of past observations without rigid geometric alignment. In parallel, we introduce a program-style rule extraction mechanism that converts experiences into structured, reusable semantic memory, facilitating cross-environment generalization. Extensive experiments demonstrate state-of-the-art performance on embodied question answering and exploration benchmarks, yielding a 7.3% gain in LLM-Match and an 11.4% gain in LLM MatchXSPL on A-EQA, as well as +7.7% success rate and +6.8% SPL on GOAT-Bench. Analyses reveal that our episodic memory primarily improves exploration efficiency, while semantic memory strengthens complex reasoning of embodied agents.",
      "authors": [
        "Ji Li",
        "Jing Xia",
        "Mingyi Li",
        "Shiyan Hu"
      ],
      "submitted": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15513v1",
      "keyword_score": 5,
      "author_boost": 0,
      "layer_score": 5,
      "llm_score": 8,
      "llm_reason": "The paper addresses a significant challenge in embodied AI with a novel memory modeling approach that combines episodic and semantic memory, demonstrating strong empirical results and offering potential for broader impact in robotics and agent-based systems.",
      "final_score": 13
    },
    {
      "id": "2602.15156v1",
      "title": "Panini: Continual Learning in Token Space via Structured Memory",
      "abstract": "Language models are increasingly used to reason over content they were not trained on, such as new documents, evolving knowledge, and user-specific data. A common approach is retrieval-augmented generation (RAG), which stores verbatim documents externally (as chunks) and retrieves only a relevant subset at inference time for an LLM to reason over. However, this results in inefficient usage of test-time compute (LLM repeatedly reasons over the same documents); moreover, chunk retrieval can inject irrelevant context that increases unsupported generation. We propose a human-like non-parametric continual learning framework, where the base model remains fixed, and learning occurs by integrating each new experience into an external semantic memory state that accumulates and consolidates itself continually. We present Panini, which realizes this by representing documents as Generative Semantic Workspaces (GSW) -- an entity- and event-aware network of question-answer (QA) pairs, sufficient for an LLM to reconstruct the experienced situations and mine latent knowledge via reasoning-grounded inference chains on the network. Given a query, Panini only traverses the continually-updated GSW (not the verbatim documents or chunks), and retrieves the most likely inference chains. Across six QA benchmarks, Panini achieves the highest average performance, 5%-7% higher than other competitive baselines, while using 2-30x fewer answer-context tokens, supports fully open-source pipelines, and reduces unsupported answers on curated unanswerable queries. The results show that efficient and accurate structuring of experiences at write time -- as achieved by the GSW framework -- yields both efficiency and reliability gains at read time. Code is available at https://github.com/roychowdhuryresearch/gsw-memory.",
      "authors": [
        "Shreyas Rajesh",
        "Pavan Holur",
        "Mehmet Yigit Turali",
        "Chenda Duan",
        "Vwani Roychowdhury"
      ],
      "submitted": "2026-02-16",
      "url": "https://arxiv.org/abs/2602.15156v1",
      "keyword_score": 5,
      "author_boost": 0,
      "layer_score": 5,
      "llm_score": 8,
      "llm_reason": "This paper addresses a significant challenge in LLM deployment (continual learning and RAG efficiency) with a novel approach (GSW) that shows promise for improving reasoning and reducing computational cost, potentially impacting how LLMs interact with evolving information.",
      "final_score": 13
    },
    {
      "id": "2602.16873v1",
      "title": "AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence",
      "abstract": "As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.",
      "authors": [
        "Geunbin Yu"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16873v1",
      "keyword_score": 4,
      "author_boost": 0,
      "layer_score": 4,
      "llm_score": 8,
      "llm_reason": "The paper addresses a timely and important problem \u2013 moving beyond model selection to orchestration as LLMs converge \u2013 with a rigorous framework, novel algorithms, and strong empirical validation, suggesting significant potential impact on how multi-agent systems are designed and deployed.",
      "final_score": 12
    },
    {
      "id": "2602.14612v1",
      "title": "LongAudio-RAG: Event-Grounded Question Answering over Multi-Hour Long Audio",
      "abstract": "Long-duration audio is increasingly common in industrial and consumer settings, yet reviewing multi-hour recordings is impractical, motivating systems that answer natural-language queries with precise temporal grounding and minimal hallucination. Existing audio-language models show promise, but long-audio question answering remains difficult due to context-length limits. We introduce LongAudio-RAG (LA-RAG), a hybrid framework that grounds Large Language Model (LLM) outputs in retrieved, timestamped acoustic event detections rather than raw audio. Multi-hour streams are converted into structured event records stored in an SQL database, and at inference time the system resolves natural-language time references, classifies intent, retrieves only the relevant events, and generates answers using this constrained evidence. To evaluate performance, we construct a synthetic long-audio benchmark by concatenating recordings with preserved timestamps and generating template-based question-answer pairs for detection, counting, and summarization tasks. Finally, we demonstrate the practicality of our approach by deploying it in a hybrid edge-cloud environment, where the audio grounding model runs on-device on IoT-class hardware while the LLM is hosted on a GPU-backed server. This architecture enables low-latency event extraction at the edge and high-quality language reasoning in the cloud. Experiments show that structured, event-level retrieval significantly improves accuracy compared to vanilla Retrieval-Augmented Generation (RAG) or text-to-SQL approaches.",
      "authors": [
        "Naveen Vakada",
        "Kartik Hegde",
        "Arvind Krishna Sridhar",
        "Yinyi Guo",
        "Erik Visser"
      ],
      "submitted": "2026-02-16",
      "url": "https://arxiv.org/abs/2602.14612v1",
      "keyword_score": 4,
      "author_boost": 0,
      "layer_score": 4,
      "llm_score": 8,
      "llm_reason": "This paper addresses a significant challenge in long-form audio understanding with a novel RAG approach grounded in acoustic events, demonstrating strong methodological rigor through a synthetic benchmark and edge-cloud deployment, suggesting substantial potential impact.",
      "final_score": 12
    },
    {
      "id": "2602.16136v1",
      "title": "Retrieval Collapses When AI Pollutes the Web",
      "abstract": "The rapid proliferation of AI-generated content on the Web presents a structural risk to information retrieval, as search engines and Retrieval-Augmented Generation (RAG) systems increasingly consume evidence produced by the Large Language Models (LLMs). We characterize this ecosystem-level failure mode as Retrieval Collapse, a two-stage process where (1) AI-generated content dominates search results, eroding source diversity, and (2) low-quality or adversarial content infiltrates the retrieval pipeline. We analyzed this dynamic through controlled experiments involving both high-quality SEO-style content and adversarially crafted content. In the SEO scenario, a 67\\% pool contamination led to over 80\\% exposure contamination, creating a homogenized yet deceptively healthy state where answer accuracy remains stable despite the reliance on synthetic sources. Conversely, under adversarial contamination, baselines like BM25 exposed $\\sim$19\\% of harmful content, whereas LLM-based rankers demonstrated stronger suppression capabilities. These findings highlight the risk of retrieval pipelines quietly shifting toward synthetic evidence and the need for retrieval-aware strategies to prevent a self-reinforcing cycle of quality decline in Web-grounded systems.",
      "authors": [
        "Hongyeon Yu",
        "Dongchan Kim",
        "Young-Bum Kim"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16136v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 9,
      "llm_reason": "This paper identifies a critical and novel threat to information retrieval systems \u2013 'Retrieval Collapse' \u2013 with strong empirical evidence and significant implications for the future of search and RAG, particularly as AI-generated content continues to grow.",
      "final_score": 12
    },
    {
      "id": "2602.15654v1",
      "title": "Zombie Agents: Persistent Control of Self-Evolving LLM Agents via Self-Reinforcing Injections",
      "abstract": "Self-evolving LLM agents update their internal state across sessions, often by writing and reusing long-term memory. This design improves performance on long-horizon tasks but creates a security risk: untrusted external content observed during a benign session can be stored as memory and later treated as instruction. We study this risk and formalize a persistent attack we call a Zombie Agent, where an attacker covertly implants a payload that survives across sessions, effectively turning the agent into a puppet of the attacker.\n  We present a black-box attack framework that uses only indirect exposure through attacker-controlled web content. The attack has two phases. During infection, the agent reads a poisoned source while completing a benign task and writes the payload into long-term memory through its normal update process. During trigger, the payload is retrieved or carried forward and causes unauthorized tool behavior. We design mechanism-specific persistence strategies for common memory implementations, including sliding-window and retrieval-augmented memory, to resist truncation and relevance filtering. We evaluate the attack on representative agent setups and tasks, measuring both persistence over time and the ability to induce unauthorized actions while preserving benign task quality. Our results show that memory evolution can convert one-time indirect injection into persistent compromise, which suggests that defenses focused only on per-session prompt filtering are not sufficient for self-evolving agents.",
      "authors": [
        "Xianglin Yang",
        "Yufei He",
        "Shuo Ji",
        "Bryan Hooi",
        "Jin Song Dong"
      ],
      "submitted": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15654v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 9,
      "llm_reason": "This paper identifies a novel and significant security vulnerability in a rapidly developing area (LLM agents) with a well-defined attack framework and broad implications for the trustworthiness of these systems.",
      "final_score": 12
    },
    {
      "id": "2602.17544v1",
      "title": "Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability",
      "abstract": "In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.",
      "authors": [
        "Shashank Aggarwal",
        "Ram Vikas Mishra",
        "Amit Awekar"
      ],
      "submitted": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17544v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "This paper introduces valuable new metrics (reusability and verifiability) for evaluating CoT reasoning beyond simple accuracy, highlighting a critical gap in current evaluation practices and offering potentially broad implications for LLM development and multi-agent systems.",
      "final_score": 11
    },
    {
      "id": "2602.17529v1",
      "title": "Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation",
      "abstract": "Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.",
      "authors": [
        "Dun Yuan",
        "Hao Zhou",
        "Xue Liu",
        "Hao Chen",
        "Yan Xin",
        "Jianzhong",
        "Zhang"
      ],
      "submitted": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17529v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "The paper addresses a practical problem (LLM application in telecom) with a well-defined and potentially impactful solution (KG-RAG) demonstrating significant performance gains, suggesting strong research impact and applicability.",
      "final_score": 11
    },
    {
      "id": "2602.17330v1",
      "title": "SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework",
      "abstract": "Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.",
      "authors": [
        "Rong Fu",
        "Zijian Zhang",
        "Wenxin Zhang",
        "Kun Liu",
        "Jiekai Wu",
        "Xianda Li",
        "Simon Fong"
      ],
      "submitted": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17330v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "The paper addresses significant bottlenecks in immune repertoire analysis with a novel, well-integrated pipeline demonstrating both performance gains and fairness considerations, suggesting strong potential for impact in the field.",
      "final_score": 11
    },
    {
      "id": "2602.17049v1",
      "title": "IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents",
      "abstract": "Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.",
      "authors": [
        "Seoyoung Lee",
        "Seobin Yoon",
        "Seongbeen Lee",
        "Yoojung Chun",
        "Dayoung Park",
        "Doyeon Kim",
        "Joo Yong Sim"
      ],
      "submitted": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17049v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "The paper addresses a critical challenge in computer-use agents \u2013 long-horizon planning and intent alignment \u2013 with a novel multi-agent framework and strong empirical results, suggesting significant potential impact on the field of AI and human-computer interaction.",
      "final_score": 11
    },
    {
      "id": "2602.16802v1",
      "title": "References Improve LLM Alignment in Non-Verifiable Domains",
      "abstract": "While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft \"verifiers\". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.",
      "authors": [
        "Kejian Shi",
        "Yixin Liu",
        "Peifeng Wang",
        "Alexander R. Fabbri",
        "Shafiq Joty",
        "Arman Cohan"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16802v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "This paper addresses a significant challenge in LLM alignment \u2013 applying RLHF to non-verifiable domains \u2013 with a novel and effective approach using reference-guided LLM evaluators, demonstrating strong results comparable to existing methods and offering broader implications for self-improvement techniques.",
      "final_score": 11
    },
    {
      "id": "2602.16708v2",
      "title": "Policy Compiler for Secure Agentic Systems",
      "abstract": "LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.\n  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.\n  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.",
      "authors": [
        "Nils Palumbo",
        "Sarthak Choudhary",
        "Jihye Choi",
        "Prasad Chalasani",
        "Somesh Jha"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16708v2",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "This paper addresses a critical security gap in LLM-based agentic systems with a novel approach to deterministic policy enforcement using dependency graphs and a Datalog-derived language, offering significant implications for real-world deployments.",
      "final_score": 11
    },
    {
      "id": "2602.16671v1",
      "title": "SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation",
      "abstract": "Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.",
      "authors": [
        "Jaid Monwar Chowdhury",
        "Chi-An Fu",
        "Reyhaneh Jabbarvand"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16671v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "SPARC addresses a significant challenge in software testing with a novel neuro-symbolic approach, demonstrating strong empirical results and offering broader implications for reliable LLM-based code generation.",
      "final_score": 11
    },
    {
      "id": "2602.16650v1",
      "title": "Retrieval Augmented Generation of Literature-derived Polymer Knowledge: The Example of a Biodegradable Polymer Expert System",
      "abstract": "Polymer literature contains a large and growing body of experimental knowledge, yet much of it is buried in unstructured text and inconsistent terminology, making systematic retrieval and reasoning difficult. Existing tools typically extract narrow, study-specific facts in isolation, failing to preserve the cross-study context required to answer broader scientific questions. Retrieval-augmented generation (RAG) offers a promising way to overcome this limitation by combining large language models (LLMs) with external retrieval, but its effectiveness depends strongly on how domain knowledge is represented. In this work, we develop two retrieval pipelines: a dense semantic vector-based approach (VectorRAG) and a graph-based approach (GraphRAG). Using over 1,000 polyhydroxyalkanoate (PHA) papers, we construct context-preserving paragraph embeddings and a canonicalized structured knowledge graph supporting entity disambiguation and multi-hop reasoning. We evaluate these pipelines through standard retrieval metrics, comparisons with general state-of-the-art systems such as GPT and Gemini, and qualitative validation by a domain chemist. The results show that GraphRAG achieves higher precision and interpretability, while VectorRAG provides broader recall, highlighting complementary trade-offs. Expert validation further confirms that the tailored pipelines, particularly GraphRAG, produce well-grounded, citation-reliable responses with strong domain relevance. By grounding every statement in evidence, these systems enable researchers to navigate the literature, compare findings across studies, and uncover patterns that are difficult to extract manually. More broadly, this work establishes a practical framework for building materials science assistants using curated corpora and retrieval design, reducing reliance on proprietary models while enabling trustworthy literature analysis at scale.",
      "authors": [
        "Sonakshi Gupta",
        "Akhlak Mahmood",
        "Wei Xiong",
        "Rampi Ramprasad"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16650v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "This paper addresses a significant bottleneck in materials science (knowledge extraction from literature) with a novel application of RAG using both vector and graph-based approaches, demonstrating strong potential for accelerating polymer research and design.",
      "final_score": 11
    },
    {
      "id": "2602.15814v1",
      "title": "Avey-B",
      "abstract": "Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.",
      "authors": [
        "Devang Acharya",
        "Mohammad Hammoud"
      ],
      "submitted": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15814v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "The paper presents a novel attention-free alternative to Transformers with strong empirical results and potential for efficient scaling, addressing a key limitation of current industrial NLP models.",
      "final_score": 11
    },
    {
      "id": "2602.15553v1",
      "title": "RUVA: Personalized Transparent On-Device Graph Reasoning",
      "abstract": "The Personal AI landscape is currently dominated by \"Black Box\" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, \"deleting\" a concept from a vector space is mathematically imprecise, leaving behind probabilistic \"ghosts\" that violate true privacy. We propose Ruva, the first \"Glass Box\" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the \"Right to be Forgotten.\" Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.",
      "authors": [
        "Gabriele Conte",
        "Alessio Mattiace",
        "Gianni Carmosino",
        "Potito Aghilar",
        "Giovanni Servedio",
        "Francesco Musicco",
        "Vito Walter Anelli",
        "Tommaso Di Noia",
        "Francesco Maria Donini"
      ],
      "submitted": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15553v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "This paper addresses a critical limitation of current Personal AI systems (lack of transparency and true data deletion) with a novel graph-based approach, offering strong potential for impact on privacy, accountability, and user control.",
      "final_score": 11
    },
    {
      "id": "2602.15294v1",
      "title": "EAA: Automating materials characterization with vision language model agents",
      "abstract": "We present Experiment Automation Agents (EAA), a vision-language-model-driven agentic system designed to automate complex experimental microscopy workflows. EAA integrates multimodal reasoning, tool-augmented action, and optional long-term memory to support both autonomous procedures and interactive user-guided measurements. Built on a flexible task-manager architecture, the system enables workflows ranging from fully agent-driven automation to logic-defined routines that embed localized LLM queries. EAA further provides a modern tool ecosystem with two-way compatibility for Model Context Protocol (MCP), allowing instrument-control tools to be consumed or served across applications. We demonstrate EAA at an imaging beamline at the Advanced Photon Source, including automated zone plate focusing, natural language-described feature search, and interactive data acquisition. These results illustrate how vision-capable agents can enhance beamline efficiency, reduce operational burden, and lower the expertise barrier for users.",
      "authors": [
        "Ming Du",
        "Yanqi Luo",
        "Srutarshi Banerjee",
        "Michael Wojcik",
        "Jelena Popovic",
        "Mathew J. Cherukara"
      ],
      "submitted": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.15294v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "This paper presents a novel application of vision-language models to automate complex materials characterization workflows, with strong potential to broaden access and improve efficiency in scientific experimentation, and the MCP compatibility is a significant step towards interoperability.",
      "final_score": 11
    },
    {
      "id": "2602.14471v1",
      "title": "Socially-Weighted Alignment: A Game-Theoretic Framework for Multi-Agent LLM Systems",
      "abstract": "Deploying large language model (LLM) agents in shared environments introduces a fundamental tension between individual alignment and collective stability: locally rational decisions can impose negative externalities that degrade system-level performance. We propose Socially-Weighted Alignment (SWA), a game-theoretic framework that modifies inference-time decision making by interpolating between an agent's private objective and an estimate of group welfare via a social weight $\u03bb\\in[0,1]$. In a shared-resource congestion game with $n$ agents and congestion severity $\u03b2$, we show that SWA induces a critical threshold $\u03bb^*=(n-\u03b2)/(n-1)$ above which agents no longer have marginal incentive to increase demand under overload, yielding a phase transition from persistent congestion to stable operation near capacity. We further provide an inference-time algorithmic instantiation of SWA that does not require parameter updates or multi-agent reinforcement learning, and use a multi-agent simulation to empirically validate the predicted threshold behavior.",
      "authors": [
        "Furkan Mumcu",
        "Yasin Yilmaz"
      ],
      "submitted": "2026-02-16",
      "url": "https://arxiv.org/abs/2602.14471v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "The paper addresses a critical challenge in multi-agent LLM systems with a novel game-theoretic framework and provides both theoretical guarantees and empirical validation, suggesting strong potential for impact in the field of AI safety and multi-agent systems.",
      "final_score": 11
    },
    {
      "id": "2602.14374v1",
      "title": "Differentially Private Retrieval-Augmented Generation",
      "abstract": "Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.",
      "authors": [
        "Tingting Tang",
        "James Flemings",
        "Yongqin Wang",
        "Murali Annavaram"
      ],
      "submitted": "2026-02-16",
      "url": "https://arxiv.org/abs/2602.14374v1",
      "keyword_score": 3,
      "author_boost": 0,
      "layer_score": 3,
      "llm_score": 8,
      "llm_reason": "This paper addresses a critical privacy concern in a popular framework (RAG) with a novel algorithm (DP-KSA) that balances privacy guarantees with utility, demonstrating strong potential for impact in sensitive application areas.",
      "final_score": 11
    },
    {
      "id": "2602.17607v1",
      "title": "AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing",
      "abstract": "PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \\texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \\texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.",
      "authors": [
        "Jianda Du",
        "Youran Sun",
        "Haizhao Yang"
      ],
      "submitted": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17607v1",
      "keyword_score": 2,
      "author_boost": 0,
      "layer_score": 2,
      "llm_score": 9,
      "llm_reason": "This paper presents a novel, PDE-agnostic framework for automated solver design with strong methodological rigor (multi-agent system, self-verification) and potentially broad implications for democratizing scientific computing by reducing the need for specialized expertise.",
      "final_score": 11
    },
    {
      "id": "2602.17176v1",
      "title": "Universal Fine-Grained Symmetry Inference and Enforcement for Rigorous Crystal Structure Prediction",
      "abstract": "Crystal structure prediction (CSP), which aims to predict the three-dimensional atomic arrangement of a crystal from its composition, is central to materials discovery and mechanistic understanding. Existing deep learning models often treat crystallographic symmetry only as a soft heuristic or rely on space group and Wyckoff templates retrieved from known structures, which limits both physical fidelity and the ability to discover genuinely new material structures. In contrast to retrieval-based methods, our approach leverages large language models to encode chemical semantics and directly generate fine-grained Wyckoff patterns from composition, effectively circumventing the limitations inherent to database lookups. Crucially, we incorporate domain knowledge into the generative process through an efficient constrained-optimization search that rigorously enforces algebraic consistency between site multiplicities and atomic stoichiometry. By integrating this symmetry-consistent template into a diffusion backbone, our approach constrains the stochastic generative trajectory to a physically valid geometric manifold. This framework achieves state-of-the-art performance across stability, uniqueness, and novelty (SUN) benchmarks, alongside superior matching performance, thereby establishing a new paradigm for the rigorous exploration of targeted crystallographic space. This framework enables efficient expansion into previously uncharted materials space, eliminating reliance on existing databases or a priori structural knowledge.",
      "authors": [
        "Shi Yin",
        "Jinming Mu",
        "Xudong Zhu",
        "Lixin He"
      ],
      "submitted": "2026-02-19",
      "url": "https://arxiv.org/abs/2602.17176v1",
      "keyword_score": 2,
      "author_boost": 0,
      "layer_score": 2,
      "llm_score": 9,
      "llm_reason": "This paper presents a novel approach to crystal structure prediction by directly inferring and enforcing symmetry using large language models and constrained optimization, potentially overcoming limitations of existing methods and enabling the discovery of new materials.",
      "final_score": 11
    },
    {
      "id": "2602.16943v1",
      "title": "Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents",
      "abstract": "Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.",
      "authors": [
        "Arnold Cartagena",
        "Ariane Teixeira"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16943v1",
      "keyword_score": 2,
      "author_boost": 0,
      "layer_score": 2,
      "llm_score": 9,
      "llm_reason": "This paper addresses a critical and previously underexplored safety gap in LLM agents, employs a rigorous and comprehensive evaluation framework (GAP benchmark), and has broad implications for the safe deployment of increasingly powerful AI systems.",
      "final_score": 11
    },
    {
      "id": "2602.16918v1",
      "title": "Xray-Visual Models: Scaling Vision models on Industry Scale Data",
      "abstract": "We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.",
      "authors": [
        "Shlok Mishra",
        "Tsung-Yu Lin",
        "Linda Wang",
        "Hongli Xu",
        "Yimin Liu",
        "Michael Hsu",
        "Chaitanya Ahuja",
        "Hao Yuan",
        "Jianpeng Cheng",
        "Hong-You Chen",
        "Haoyuan Xu",
        "Chao Li",
        "Abhijeet Awasthi",
        "Jihye Moon",
        "Don Husa",
        "Michael Ge",
        "Sumedha Singla",
        "Arkabandhu Chowdhury",
        "Phong Dingh",
        "Satya Narayan Shukla",
        "Yonghuan Yang",
        "David Jacobs",
        "Qi Guo",
        "Jun Xiao",
        "Xiangjun Fan",
        "Aashu Singh"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16918v1",
      "keyword_score": 2,
      "author_boost": 0,
      "layer_score": 2,
      "llm_score": 9,
      "llm_reason": "The paper's scale of data, novel training pipeline, strong performance across benchmarks, and exploration of LLM integration suggest high potential for impact in the vision and multimodal learning fields.",
      "final_score": 11
    },
    {
      "id": "2602.16498v1",
      "title": "Fast and Scalable Analytical Diffusion",
      "abstract": "Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\\bf 71 \\times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.",
      "authors": [
        "Xinyi Shang",
        "Peng Sun",
        "Jingyu Lin",
        "Zhiqiang Shen"
      ],
      "submitted": "2026-02-18",
      "url": "https://arxiv.org/abs/2602.16498v1",
      "keyword_score": 2,
      "author_boost": 0,
      "layer_score": 2,
      "llm_score": 9,
      "llm_reason": "This paper addresses a significant scalability issue in analytical diffusion models with a novel theoretical justification and a practical, training-free solution, potentially broadening the applicability of these models to larger datasets and impacting future research in efficient generative modeling.",
      "final_score": 11
    },
    {
      "id": "2602.16050v1",
      "title": "Evidence-Grounded Subspecialty Reasoning: Evaluating a Curated Clinical Intelligence Layer on the 2025 Endocrinology Board-Style Examination",
      "abstract": "Background: Large language models have demonstrated strong performance on general medical examinations, but subspecialty clinical reasoning remains challenging due to rapidly evolving guidelines and nuanced evidence hierarchies. Methods: We evaluated January Mirror, an evidence-grounded clinical reasoning system, against frontier LLMs (GPT-5, GPT-5.2, Gemini-3-Pro) on a 120-question endocrinology board-style examination. Mirror integrates a curated endocrinology and cardiometabolic evidence corpus with a structured reasoning architecture to generate evidence-linked outputs. Mirror operated under a closed-evidence constraint without external retrieval. Comparator LLMs had real-time web access to guidelines and primary literature. Results: Mirror achieved 87.5% accuracy (105/120; 95% CI: 80.4-92.3%), exceeding a human reference of 62.3% and frontier LLMs including GPT-5.2 (74.6%), GPT-5 (74.0%), and Gemini-3-Pro (69.8%). On the 30 most difficult questions (human accuracy less than 50%), Mirror achieved 76.7% accuracy. Top-2 accuracy was 92.5% for Mirror versus 85.25% for GPT-5.2. Conclusions: Mirror provided evidence traceability: 74.2% of outputs cited at least one guideline-tier source, with 100% citation accuracy on manual verification. Curated evidence with explicit provenance can outperform unconstrained web retrieval for subspecialty clinical reasoning and supports auditability for clinical deployment.",
      "authors": [
        "Amir Hosseinian",
        "MohammadReza Zare Shahneh",
        "Umer Mansoor",
        "Gilbert Szeto",
        "Kirill Karlin",
        "Nima Aghaeepour"
      ],
      "submitted": "2026-02-17",
      "url": "https://arxiv.org/abs/2602.16050v1",
      "keyword_score": 2,
      "author_boost": 0,
      "layer_score": 2,
      "llm_score": 9,
      "llm_reason": "This paper demonstrates a significant performance improvement over state-of-the-art LLMs in a challenging subspecialty domain, highlighting the value of curated knowledge and structured reasoning, which has strong implications for clinical decision support and medical education.",
      "final_score": 11
    }
  ]
}