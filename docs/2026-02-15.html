<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Apollo Digest 2026-02-15</title>
  <style>
    :root {
      --bg: #0d1117;
      --surface: #161b22;
      --border: #30363d;
      --text: #c9d1d9;
      --muted: #8b949e;
      --accent: #58a6ff;
      --accent-dim: #1f6feb;
      --green: #3fb950;
      --score-high: #3fb950;
      --score-mid: #d29922;
      --score-low: #f85149;
    }
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      background: var(--bg);
      color: var(--text);
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
      line-height: 1.6;
      padding: 2rem 1rem;
    }
    a { color: var(--accent); text-decoration: none; }
    a:hover { text-decoration: underline; }
    .container { max-width: 860px; margin: 0 auto; }

    /* Header */
    header { border-bottom: 1px solid var(--border); padding-bottom: 1.5rem; margin-bottom: 2rem; }
    header .logo { font-size: 1.1rem; color: var(--muted); margin-bottom: 0.5rem; }
    header h1 { font-size: 2rem; font-weight: 700; color: var(--text); }
    header .meta { font-size: 0.85rem; color: var(--muted); margin-top: 0.5rem; }

    /* Archive nav */
    .archive { background: var(--surface); border: 1px solid var(--border); border-radius: 8px; padding: 1rem 1.25rem; margin-bottom: 2rem; }
    .archive h2 { font-size: 0.9rem; text-transform: uppercase; letter-spacing: 0.05em; color: var(--muted); margin-bottom: 0.75rem; }
    .archive ul { list-style: none; display: flex; flex-wrap: wrap; gap: 0.5rem; }
    .archive li a {
      display: inline-block;
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 4px;
      padding: 0.2rem 0.6rem;
      font-size: 0.8rem;
      color: var(--muted);
    }
    .archive li a:hover { color: var(--accent); border-color: var(--accent-dim); text-decoration: none; }
    .archive li a.current { border-color: var(--accent); color: var(--accent); }

    /* Paper card */
    .paper {
      background: var(--surface);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.25rem 1.5rem;
      margin-bottom: 1.25rem;
    }
    .paper-header { display: flex; align-items: flex-start; gap: 1rem; }
    .rank {
      flex-shrink: 0;
      width: 2.2rem;
      height: 2.2rem;
      border-radius: 50%;
      background: var(--accent-dim);
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: 700;
      font-size: 0.85rem;
      color: #fff;
    }
    .paper-title a { font-size: 1.05rem; font-weight: 600; color: var(--text); }
    .paper-title a:hover { color: var(--accent); }
    .paper-meta { margin-top: 0.35rem; font-size: 0.8rem; color: var(--muted); }

    .scores { display: flex; gap: 0.6rem; margin: 0.75rem 0; flex-wrap: wrap; }
    .badge {
      font-size: 0.75rem;
      padding: 0.2rem 0.55rem;
      border-radius: 999px;
      border: 1px solid currentColor;
      white-space: nowrap;
    }
    .badge-final { color: var(--accent); }
    .badge-llm-high { color: var(--score-high); }
    .badge-llm-mid  { color: var(--score-mid); }
    .badge-llm-low  { color: var(--score-low); }

    .llm-reason {
      font-size: 0.875rem;
      color: var(--accent);
      font-style: italic;
      margin-bottom: 0.6rem;
      border-left: 3px solid var(--accent-dim);
      padding-left: 0.75rem;
    }
    .abstract {
      font-size: 0.875rem;
      color: var(--muted);
      display: -webkit-box;
      -webkit-line-clamp: 4;
      -webkit-box-orient: vertical;
      overflow: hidden;
    }
    .arxiv-link { display: inline-block; margin-top: 0.75rem; font-size: 0.8rem; }

    footer { margin-top: 3rem; padding-top: 1rem; border-top: 1px solid var(--border); font-size: 0.8rem; color: var(--muted); text-align: center; }
  </style>
</head>
<body>
<div class="container">

  <header>
    <div class="logo">Apollo — Autonomous AI Research Analyst</div>
    <h1>
      Digest
      &mdash; 2026-02-02 to 2026-02-23
    </h1>
    <div class="meta">
      25 papers &bull; 2026-02-02 to 2026-02-23 &bull; Generated 2026-02-23 &bull;
      cs.AI &bull; <a href="https://github.com/Dhruvq/Apollo-AI-Research-Analyst">GitHub</a>
    </div>
  </header>

  

  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">1</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.18916v1" target="_blank" rel="noopener">Adaptive Collaboration of Arena-Based Argumentative LLMs for Explainable and Contestable Legal Reasoning</a>
        <div class="paper-meta">
          Hoang-Loc Cao, Phuc Ho, Truong Thanh Hung Nguyen et al.
          &bull; 2026-02-21
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 13</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 5</span>
      
    </div>

    
    <div class="llm-reason">The paper presents a novel neuro-symbolic framework addressing a critical gap in LLM legal reasoning – explainability and contestability – with strong empirical results and potential for broader impact in AI safety and legal tech.</div>
    

    <div class="abstract">Legal reasoning requires not only high accuracy but also the ability to justify decisions through verifiable and contestable arguments. However, existing Large Language Model (LLM) approaches, such as Chain-of-Thought (CoT) and Retrieval-Augmented Generation (RAG), often produce unstructured explanations that lack a formal mechanism for verification or user intervention. To address this limitation, we propose Adaptive Collaboration of Argumentative LLMs (ACAL), a neuro-symbolic framework that integrates adaptive multi-agent collaboration with an Arena-based Quantitative Bipolar Argumentation Framework (A-QBAF). ACAL dynamically deploys expert agent teams to construct arguments, employs a clash resolution mechanism to adjudicate conflicting claims, and utilizes uncertainty-aware escalation for borderline cases. Crucially, our framework supports a Human-in-the-Loop (HITL) contestability workflow, enabling users to directly audit and modify the underlying reasoning graph to influence the final judgment. Empirical evaluations on the LegalBench benchmark demonstrate that ACAL outperforms strong baselines across Gemini-2.5-Flash-Lite and Gemini-2.5-Flash architectures, effectively balancing efficient predictive performance with structured transparency and contestability. Our implementation is available at: https://github.com/loc110504/ACAL.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.18916v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.18916v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">2</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17999v1" target="_blank" rel="noopener">Aurora: Neuro-Symbolic AI Driven Advising Agent</a>
        <div class="paper-meta">
          Lorena Amanda Quincoso Lugones, Christopher Kverne, Nityam Sharadkumar Bhimani et al.
          &bull; 2026-02-20
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 13</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 5</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a significant problem with a novel neuro-symbolic approach, demonstrates strong methodological rigor through a structured evaluation, and has broad implications for improving access to higher education advising.</div>
    

    <div class="abstract">Academic advising in higher education is under severe strain, with advisor-to-student ratios commonly exceeding 300:1. These structural bottlenecks limit timely access to guidance, increase the risk of delayed graduation, and contribute to inequities in student support. We introduce Aurora, a modular neuro-symbolic advising agent that unifies retrieval-augmented generation (RAG), symbolic reasoning, and normalized curricular databases to deliver policy-compliant, verifiable recommendations at scale. Aurora integrates three components: (i) a Boyce-Codd Normal Form (BCNF) catalog schema for consistent program rules, (ii) a Prolog engine for prerequisite and credit enforcement, and (iii) an instruction-tuned large language model for natural-language explanations of its recommendations. To assess performance, we design a structured evaluation suite spanning common and edge-case advising scenarios, including short-term scheduling, long-term roadmapping, skill-aligned pathways, and out-of-scope requests. Across this diverse set, Aurora improves semantic alignment with expert-crafted answers from 0.68 (Raw LLM baseline) to 0.93 (+36%), achieves perfect precision and recall in nearly half of in-scope cases, and consistently produces correct fallbacks for unanswerable prompts. On commodity hardware, Aurora delivers sub-second mean latency (0.71s across 20 queries), approximately 83X faster than a Raw LLM baseline (59.2s). By combining symbolic rigor with neural fluency, Aurora advances a paradigm for accurate, explainable, and scalable AI-driven advising.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17999v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17999v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">3</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19534v1" target="_blank" rel="noopener">Large Language Model-Assisted UAV Operations and Communications: A Multifaceted Survey and Tutorial</a>
        <div class="paper-meta">
          Yousef Emami, Hao Zhou, Radha Reddy et al.
          &bull; 2026-02-23
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 12</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 4</span>
      
    </div>

    
    <div class="llm-reason">This survey paper addresses a highly relevant and emerging intersection of LLMs and UAVs, offering a comprehensive overview and framework that could significantly guide future research and development in the field.</div>
    

    <div class="abstract">Uncrewed Aerial Vehicles (UAVs) are widely deployed across diverse applications due to their mobility and agility. Recent advances in Large Language Models (LLMs) offer a transformative opportunity to enhance UAV intelligence beyond conventional optimization-based and learning-based approaches. By integrating LLMs into UAV systems, advanced environmental understanding, swarm coordination, mobility optimization, and high-level task reasoning can be achieved, thereby allowing more adaptive and context-aware aerial operations. This survey systematically explores the intersection of LLMs and UAV technologies and proposes a unified framework that consolidates existing architectures, methodologies, and applications for UAVs. We first present a structured taxonomy of LLM adaptation techniques for UAVs, including pretraining, fine-tuning, Retrieval-Augmented Generation (RAG), and prompt engineering, along with key reasoning capabilities such as Chain-of-Thought (CoT) and In-Context Learning (ICL). We then examine LLM-assisted UAV communications and operations, covering navigation, mission planning, swarm control, safety, autonomy, and network management. After that, the survey further discusses Multimodal LLMs (MLLMs) for human-swarm interaction, perception-driven navigation, and collaborative control. Finally, we address ethical considerations, including bias, transparency, accountability, and Human-in-the-Loop (HITL) strategies, and outline future research directions. Overall, this work positions LLM-assisted UAVs as a foundation for intelligent and adaptive aerial systems.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19534v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19534v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">4</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17902v1" target="_blank" rel="noopener">El Agente Gráfico: Structured Execution Graphs for Scientific Agents</a>
        <div class="paper-meta">
          Jiaru Bai, Abdulrahman Aldossary, Thomas Swanick et al.
          &bull; 2026-02-19
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 12</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 4</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a critical challenge in LLM-driven scientific workflows – robustness and auditability – with a novel approach using structured execution graphs and typed object mapping, demonstrating strong potential for impact in the field.</div>
    

    <div class="abstract">Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gráfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17902v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17902v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">5</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.16873v1" target="_blank" rel="noopener">AdaptOrch: Task-Adaptive Multi-Agent Orchestration in the Era of LLM Performance Convergence</a>
        <div class="paper-meta">
          Geunbin Yu
          &bull; 2026-02-18
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 12</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 4</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a timely and important problem – moving beyond model selection to orchestration as LLMs converge – with a rigorous framework, novel algorithms, and validation across diverse tasks, suggesting significant potential impact on how LLM systems are built and deployed.</div>
    

    <div class="abstract">As large language models from diverse providers converge toward comparable benchmark performance, the traditional paradigm of selecting a single best model per task yields diminishing returns. We argue that orchestration topology -- the structural composition of how multiple agents are coordinated, parallelized, and synthesized -- now dominates system-level performance over individual model capability. We present AdaptOrch, a formal framework for task-adaptive multi-agent orchestration that dynamically selects among four canonical topologies (parallel, sequential, hierarchical, and hybrid) based on task dependency graphs and empirically derived domain characteristics. Our framework introduces three key contributions: (1) a Performance Convergence Scaling Law, formalizing conditions under which orchestration selection outweighs model selection; (2) a Topology Routing Algorithm that maps task decomposition DAGs to optimal orchestration patterns in O(|V| + |E|) time; and (3) an Adaptive Synthesis Protocol with provable termination guarantees and heuristic consistency scoring for parallel agent outputs. We validate AdaptOrch across coding (SWE-bench), reasoning (GPQA), and retrieval-augmented generation tasks, demonstrating that topology-aware orchestration achieves 12-23% improvement over static single-topology baselines, even when using identical underlying models. Our results establish orchestration design as a first-class optimization target independent of model scaling.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.16873v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.16873v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">6</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19416v1" target="_blank" rel="noopener">IR$^3$: Contrastive Inverse Reinforcement Learning for Interpretable Detection and Mitigation of Reward Hacking</a>
        <div class="paper-meta">
          Mohammad Beigi, Ming Jin, Junshan Zhang et al.
          &bull; 2026-02-23
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 12</span>
      <span class="badge badge-llm-high">
        Impact: 9/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper addresses a critical and emerging problem in RLHF (reward hacking) with a novel framework (IR3) combining contrastive IRL, interpretability techniques, and targeted mitigation strategies, demonstrating strong potential for impact on the field of LLM alignment and safety.</div>
    

    <div class="abstract">Reinforcement Learning from Human Feedback (RLHF) enables powerful LLM alignment but can introduce reward hacking - models exploit spurious correlations in proxy rewards without genuine alignment. Compounding this, the objectives internalized during RLHF remain opaque, making hacking behaviors difficult to detect or correct. We introduce IR3 (Interpretable Reward Reconstruction and Rectification), a framework that reverse-engineers, interprets, and surgically repairs the implicit objectives driving RLHF-tuned models. We propose Contrastive Inverse Reinforcement Learning (C-IRL), which reconstructs the implicit reward function by contrasting paired responses from post-alignment and baseline policies to explain behavioral shifts during RLHF. We then decompose the reconstructed reward via sparse autoencoders into interpretable features, enabling identification of hacking signatures through contribution analysis. Finally, we propose mitigation strategies - clean reward optimization, adversarial shaping, constrained optimization, and feature-guided distillation - that target problematic features while preserving beneficial alignment. Experiments across multiple reward model configurations show that IR3 achieves 0.89 correlation with ground-truth rewards, identifies hacking features with over 90% precision, and significantly reduces hacking behaviors while maintaining capabilities within 3% of the original model.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19416v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19416v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">7</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.20135v1" target="_blank" rel="noopener">KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration</a>
        <div class="paper-meta">
          Mohammad Amanlou, Erfan Shafiee Moghaddam, Yasaman Amou Jafari et al.
          &bull; 2026-02-23
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a critical bottleneck in LLM evaluation with a novel knowledge graph approach to MCQ generation, offering good methodological rigor and broader implications for efficient and controllable dataset creation.</div>
    

    <div class="abstract">With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.20135v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.20135v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">8</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19843v1" target="_blank" rel="noopener">MAS-FIRE: Fault Injection and Reliability Evaluation for LLM-Based Multi-Agent Systems</a>
        <div class="paper-meta">
          Jin Jia, Zhiling Deng, Zhuangbin Chen et al.
          &bull; 2026-02-23
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper addresses a critical and emerging problem in LLM-based MAS reliability with a novel fault injection framework and insightful tiered analysis, offering significant implications for building robust multi-agent systems.</div>
    

    <div class="abstract">As LLM-based Multi-Agent Systems (MAS) are increasingly deployed for complex tasks, ensuring their reliability has become a pressing challenge. Since MAS coordinate through unstructured natural language rather than rigid protocols, they are prone to semantic failures (e.g., hallucinations, misinterpreted instructions, and reasoning drift) that propagate silently without raising runtime exceptions. Prevailing evaluation approaches, which measure only end-to-end task success, offer limited insight into how these failures arise or how effectively agents recover from them. To bridge this gap, we propose MAS-FIRE, a systematic framework for fault injection and reliability evaluation of MAS. We define a taxonomy of 15 fault types covering intra-agent cognitive errors and inter-agent coordination failures, and inject them via three non-invasive mechanisms: prompt modification, response rewriting, and message routing manipulation. Applying MAS-FIRE to three representative MAS architectures, we uncover a rich set of fault-tolerant behaviors that we organize into four tiers: mechanism, rule, prompt, and reasoning. This tiered view enables fine-grained diagnosis of where and why systems succeed or fail. Our findings reveal that stronger foundation models do not uniformly improve robustness. We further show that architectural topology plays an equally decisive role, with iterative, closed-loop designs neutralizing over 40% of faults that cause catastrophic collapse in linear workflows. MAS-FIRE provides the process-level observability and actionable guidance needed to systematically improve multi-agent systems.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19843v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19843v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">9</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19620v1" target="_blank" rel="noopener">Rules or Weights? Comparing User Understanding of Explainable AI Techniques with the Cognitive XAI-Adaptive Model</a>
        <div class="paper-meta">
          Louth Bin Rawshan, Zhuoyu Wang, Brian Y Lim
          &bull; 2026-02-23
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper presents a novel cognitive model (CoXAM) to compare XAI techniques, grounded in a user study and validated against human decision-making, offering significant implications for the design and evaluation of explainable AI systems.</div>
    

    <div class="abstract">Rules and Weights are popular XAI techniques for explaining AI decisions. Yet, it remains unclear how to choose between them, lacking a cognitive framework to compare their interpretability. In an elicitation user study on forward and counterfactual decision tasks, we identified 7 reasoning strategies of interpreting three XAI Schemas - weights, rules, and their hybrid. To analyze their capabilities, we propose CoXAM, a Cognitive XAI-Adaptive Model with shared memory representation to encode instance attributes, linear weights, and decision rules. CoXAM employs computational rationality to choose among reasoning processes based on the trade-off in utility and reasoning time, separately for forward or counterfactual decision tasks. In a validation study, CoXAM demonstrated a stronger alignment with human decision-making compared to baseline machine learning proxy models. The model successfully replicated and explained several key empirical findings, including that counterfactual tasks are inherently harder than forward tasks, decision tree rules are harder to recall and apply than linear weights, and the helpfulness of XAI depends on the application data context, alongside identifying which underlying reasoning strategies were most effective. With CoXAM, we contribute a cognitive basis to accelerate debugging and benchmarking disparate XAI techniques.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19620v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19620v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">10</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19320v1" target="_blank" rel="noopener">Anatomy of Agentic Memory: Taxonomy and Empirical Analysis of Evaluation and System Limitations</a>
        <div class="paper-meta">
          Dongming Jiang, Yi Li, Songtao Wei et al.
          &bull; 2026-02-22
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper provides a crucial, comprehensive survey and analysis of a rapidly evolving field (agentic memory) identifying key limitations and offering a structured taxonomy, which will likely guide future research and development efforts.</div>
    

    <div class="abstract">Agentic memory systems enable large language model (LLM) agents to maintain state across long interactions, supporting long-horizon reasoning and personalization beyond fixed context windows. Despite rapid architectural development, the empirical foundations of these systems remain fragile: existing benchmarks are often underscaled, evaluation metrics are misaligned with semantic utility, performance varies significantly across backbone models, and system-level costs are frequently overlooked. This survey presents a structured analysis of agentic memory from both architectural and system perspectives. We first introduce a concise taxonomy of MAG systems based on four memory structures. Then, we analyze key pain points limiting current systems, including benchmark saturation effects, metric validity and judge sensitivity, backbone-dependent accuracy, and the latency and throughput overhead introduced by memory maintenance. By connecting the memory structure to empirical limitations, this survey clarifies why current agentic memory systems often underperform their theoretical promise and outlines directions for more reliable evaluation and scalable system design.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19320v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19320v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">11</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19317v1" target="_blank" rel="noopener">Learning to Reason for Multi-Step Retrieval of Personal Context in Personalized Question Answering</a>
        <div class="paper-meta">
          Maryam Amirizaniani, Alireza Salemi, Hamed Zamani
          &bull; 2026-02-22
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a crucial challenge in personalized QA with a novel reinforcement learning approach to adaptive retrieval and reasoning, demonstrating strong empirical results on a relevant benchmark, suggesting significant potential impact.</div>
    

    <div class="abstract">Personalization in Question Answering (QA) requires answers that are both accurate and aligned with users&#39; background, preferences, and historical context. Existing state-of-the-art methods primarily rely on retrieval-augmented generation (RAG) solutions that construct personal context by retrieving relevant items from the user&#39;s profile. Existing methods use the user&#39;s query directly to retrieve personal documents, and such strategies often lead to surface-level personalization. We propose PR2 (Personalized Retrieval-Augmented Reasoning), a reinforcement learning framework that integrates reasoning and retrieval from personal context for personalization. PR2 learns adaptive retrieval-reasoning policies, determining when to retrieve, what evidence to retrieve from user profiles, and how to incorporate it into intermediate reasoning steps. By optimizing multi-turn reasoning trajectories under a personalized reward function, the framework reinforces reasoning paths that better align with user-specific preferences and contextual signals reflected by the reward model. Extensive experiments on the LaMP-QA benchmark using three LLMs show that PR2 consistently outperforms strong baselines, achieving an average relative improvement of 8.8%-12% in personalized QA.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19317v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19317v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">12</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19240v1" target="_blank" rel="noopener">Topology of Reasoning: Retrieved Cell Complex-Augmented Generation for Textual Graph Question Answering</a>
        <div class="paper-meta">
          Sen Zhao, Lincheng Zhou, Yue Chen et al.
          &bull; 2026-02-22
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a significant limitation in existing RAG methods for graph question answering by incorporating topological information, offering a novel approach with potentially broad implications for reasoning over complex relational data.</div>
    

    <div class="abstract">Retrieval-Augmented Generation (RAG) enhances the reasoning ability of Large Language Models (LLMs) by dynamically integrating external knowledge, thereby mitigating hallucinations and strengthening contextual grounding for structured data such as graphs. Nevertheless, most existing RAG variants for textual graphs concentrate on low-dimensional structures -- treating nodes as entities (0-dimensional) and edges or paths as pairwise or sequential relations (1-dimensional), but overlook cycles, which are crucial for reasoning over relational loops. Such cycles often arise in questions requiring closed-loop inference about similar objects or relative positions. This limitation often results in incomplete contextual grounding and restricted reasoning capability. In this work, we propose Topology-enhanced Retrieval-Augmented Generation (TopoRAG), a novel framework for textual graph question answering that effectively captures higher-dimensional topological and relational dependencies. Specifically, TopoRAG first lifts textual graphs into cellular complexes to model multi-dimensional topological structures. Leveraging these lifted representations, a topology-aware subcomplex retrieval mechanism is proposed to extract cellular complexes relevant to the input query, providing compact and informative topological context. Finally, a multi-dimensional topological reasoning mechanism operates over these complexes to propagate relational information and guide LLMs in performing structured, logic-aware inference. Empirical evaluations demonstrate that our method consistently surpasses existing baselines across diverse textual graph tasks.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19240v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19240v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">13</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.19159v1" target="_blank" rel="noopener">Beyond Behavioural Trade-Offs: Mechanistic Tracing of Pain-Pleasure Decisions in an LLM</a>
        <div class="paper-meta">
          Francesca Bianco, Derek Shiller
          &bull; 2026-02-22
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper combines behavioral observations with rigorous mechanistic interpretability techniques to understand how LLMs process valence, offering a strong contribution to the field of AI alignment and interpretability with potentially broad implications for understanding decision-making in these models.</div>
    

    <div class="abstract">Prior behavioural work suggests that some LLMs alter choices when options are framed as causing pain or pleasure, and that such deviations can scale with stated intensity. To bridge behavioural evidence (what the model does) with mechanistic interpretability (what computations support it), we investigate how valence-related information is represented and where it is causally used inside a transformer. Using Gemma-2-9B-it and a minimalist decision task modelled on prior work, we (i) map representational availability with layer-wise linear probing across streams, (ii) test causal contribution with activation interventions (steering; patching/ablation), and (iii) quantify dose-response effects over an epsilon grid, reading out both the 2-3 logit margin and digit-pair-normalised choice probabilities. We find that (a) valence sign (pain vs. pleasure) is perfectly linearly separable across stream families from very early layers (L0-L1), while a lexical baseline retains substantial signal; (b) graded intensity is strongly decodable, with peaks in mid-to-late layers and especially in attention/MLP outputs, and decision alignment is highest slightly before the final token; (c) additive steering along a data-derived valence direction causally modulates the 2-3 margin at late sites, with the largest effects observed in late-layer attention outputs (attn_out L14); and (d) head-level patching/ablation suggests that these effects are distributed across multiple heads rather than concentrated in a single unit. Together, these results link behavioural sensitivity to identifiable internal representations and intervention-sensitive sites, providing concrete mechanistic targets for more stringent counterfactual tests and broader replication. This work supports a more evidence-driven (a) debate on AI sentience and welfare, and (b) governance when setting policy, auditing standards, and safety safeguards.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.19159v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.19159v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">14</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.18929v1" target="_blank" rel="noopener">Give Users the Wheel: Towards Promptable Recommendation Paradigm</a>
        <div class="paper-meta">
          Fuyuan Lyu, Chenglin Luo, Qiyuan Zhang et al.
          &bull; 2026-02-21
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper addresses a significant limitation of current recommendation systems by effectively integrating LLMs for prompt-based control without sacrificing efficiency or collaborative filtering, demonstrating strong novelty and potential for broad impact.</div>
    

    <div class="abstract">Conventional sequential recommendation models have achieved remarkable success in mining implicit behavioral patterns. However, these architectures remain structurally blind to explicit user intent: they struggle to adapt when a user&#39;s immediate goal (e.g., expressed via a natural language prompt) deviates from their historical habits. While Large Language Models (LLMs) offer the semantic reasoning to interpret such intent, existing integration paradigms force a dilemma: LLM-as-a-recommender paradigm sacrifices the efficiency and collaborative precision of ID-based retrieval, while Reranking methods are inherently bottlenecked by the recall capabilities of the underlying model. In this paper, we propose Decoupled Promptable Sequential Recommendation (DPR), a model-agnostic framework that empowers conventional sequential backbones to natively support Promptable Recommendation, the ability to dynamically steer the retrieval process using natural language without abandoning collaborative signals. DPR modulates the latent user representation directly within the retrieval space. To achieve this, we introduce a Fusion module to align the collaborative and semantic signals, a Mixture-of-Experts (MoE) architecture that disentangles the conflicting gradients from positive and negative steering, and a three-stage training strategy that progressively aligns the semantic space of prompts with the collaborative space. Extensive experiments on real-world datasets demonstrate that DPR significantly outperforms state-of-the-art baselines in prompt-guided tasks while maintaining competitive performance in standard sequential recommendation scenarios.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.18929v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.18929v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">15</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.18568v1" target="_blank" rel="noopener">RPU -- A Reasoning Processing Unit</a>
        <div class="paper-meta">
          Matthew Adiletta, Gu-Yeon Wei, David Brooks
          &bull; 2026-02-20
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a critical bottleneck in LLM inference with a novel architecture (RPU) and specific optimizations (HBM-CO, chiplet design, decoupled pipelines) showing significant performance gains, suggesting strong potential impact on future LLM hardware.</div>
    

    <div class="abstract">Large language model (LLM) inference performance is increasingly bottlenecked by the memory wall. While GPUs continue to scale raw compute throughput, they struggle to deliver scalable performance for memory bandwidth bound workloads. This challenge is amplified by emerging reasoning LLM applications, where long output sequences, low arithmetic intensity, and tight latency constraints demand significantly higher memory bandwidth. As a result, system utilization drops and energy per inference rises, highlighting the need for an optimized system architecture for scalable memory bandwidth.
  To address these challenges we present the Reasoning Processing Unit (RPU), a chiplet-based architecture designed to address the challenges of the modern memory wall. RPU introduces: (1) A Capacity-Optimized High-Bandwidth Memory (HBM-CO) that trades capacity for lower energy and cost; (2) a scalable chiplet architecture featuring a bandwidth-first power and area provisioning design; and (3) a decoupled microarchitecture that separates memory, compute, and communication pipelines to sustain high bandwidth utilization. Simulation results show that RPU performs up to 45.3x lower latency and 18.6x higher throughput over an H100 system at ISO-TDP on Llama3-405B.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.18568v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.18568v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">16</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17949v1" target="_blank" rel="noopener">CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications</a>
        <div class="paper-meta">
          Victoria Blake, Mathew Miller, Jamie Novak et al.
          &bull; 2026-02-20
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a significant bottleneck in clinical NLP (concept set curation) with a novel GraphRAG approach, demonstrates strong performance against a gold standard, and has clear implications for improving downstream task accuracy and efficiency.</div>
    

    <div class="abstract">Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17949v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17949v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">17</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17910v1" target="_blank" rel="noopener">Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems</a>
        <div class="paper-meta">
          Hanjing Shi, Dominic DiFranzo
          &bull; 2026-02-20
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a crucial and relatively unexplored aspect of AI alignment – sustained reliability in long-horizon agents – with a novel, practical approach (APEMO) and strong empirical validation, suggesting significant potential impact.</div>
    

    <div class="abstract">Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17910v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17910v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">18</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17544v1" target="_blank" rel="noopener">Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability</a>
        <div class="paper-meta">
          Shashank Aggarwal, Ram Vikas Mishra, Amit Awekar
          &bull; 2026-02-19
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper introduces valuable new metrics (reusability and verifiability) for evaluating CoT reasoning beyond simple accuracy, highlighting a critical gap in current evaluation practices and offering potentially broad implications for multi-agent LLM systems.</div>
    

    <div class="abstract">In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker&#39;s CoT. Verifiability measures how frequently an Executor can match the Thinker&#39;s answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17544v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17544v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">19</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17529v1" target="_blank" rel="noopener">Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation</a>
        <div class="paper-meta">
          Dun Yuan, Hao Zhou, Xue Liu et al.
          &bull; 2026-02-19
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a practical problem (LLM application in telecom) with a well-defined and potentially impactful solution (KG-RAG) demonstrating significant performance gains, suggesting strong research impact and applicability.</div>
    

    <div class="abstract">Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model&#39;s outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG&#39;s effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17529v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17529v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">20</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17330v1" target="_blank" rel="noopener">SubQuad: Near-Quadratic-Free Structure Inference with Distribution-Balanced Objectives in Adaptive Receptor framework</a>
        <div class="paper-meta">
          Rong Fu, Zijian Zhang, Wenxin Zhang et al.
          &bull; 2026-02-19
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses significant bottlenecks in immune repertoire analysis with a novel, well-integrated pipeline demonstrating both performance gains and fairness considerations, suggesting strong potential for impact in the field.</div>
    

    <div class="abstract">Comparative analysis of adaptive immune repertoires at population scale is hampered by two practical bottlenecks: the near-quadratic cost of pairwise affinity evaluations and dataset imbalances that obscure clinically important minority clonotypes. We introduce SubQuad, an end-to-end pipeline that addresses these challenges by combining antigen-aware, near-subquadratic retrieval with GPU-accelerated affinity kernels, learned multimodal fusion, and fairness-constrained clustering. The system employs compact MinHash prefiltering to sharply reduce candidate comparisons, a differentiable gating module that adaptively weights complementary alignment and embedding channels on a per-pair basis, and an automated calibration routine that enforces proportional representation of rare antigen-specific subgroups. On large viral and tumor repertoires SubQuad achieves measured gains in throughput and peak memory usage while preserving or improving recall@k, cluster purity, and subgroup equity. By co-designing indexing, similarity fusion, and equity-aware objectives, SubQuad offers a scalable, bias-aware platform for repertoire mining and downstream translational tasks such as vaccine target prioritization and biomarker discovery.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17330v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17330v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">21</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.17049v1" target="_blank" rel="noopener">IntentCUA: Learning Intent-level Representations for Skill Abstraction and Multi-Agent Planning in Computer-Use Agents</a>
        <div class="paper-meta">
          Seoyoung Lee, Seobin Yoon, Seongbeen Lee et al.
          &bull; 2026-02-19
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">The paper addresses a significant challenge in computer-use agents – long-horizon planning and intent alignment – with a novel multi-agent framework and strong empirical results, suggesting substantial potential impact on the field.</div>
    

    <div class="abstract">Computer-use agents operate over long horizons under noisy perception, multi-window contexts, evolving environment states. Existing approaches, from RL-based planners to trajectory retrieval, often drift from user intent and repeatedly solve routine subproblems, leading to error accumulation and inefficiency. We present IntentCUA, a multi-agent computer-use framework designed to stabilize long-horizon execution through intent-aligned plan memory. A Planner, Plan-Optimizer, and Critic coordinate over shared memory that abstracts raw interaction traces into multi-view intent representations and reusable skills. At runtime, intent prototypes retrieve subgroup-aligned skills and inject them into partial plans, reducing redundant re-planning and mitigating error propagation across desktop applications. In end-to-end evaluations, IntentCUA achieved a 74.83% task success rate with a Step Efficiency Ratio of 0.91, outperforming RL-based and trajectory-centric baselines. Ablations show that multi-view intent abstraction and shared plan memory jointly improve execution stability, with the cooperative multi-agent loop providing the largest gains on long-horizon tasks. These results highlight that system-level intent abstraction and memory-grounded coordination are key to reliable and efficient desktop automation in large, dynamic environments.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.17049v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.17049v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">22</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.16898v2" target="_blank" rel="noopener">MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation</a>
        <div class="paper-meta">
          Iman Ahmadi, Mehrshad Taji, Arad Mahdinezhad Kashani et al.
          &bull; 2026-02-18
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper presents a novel multi-agent framework leveraging LLMs and VLMs for robust robotic manipulation with closed-loop feedback, demonstrating strong potential for advancing the field beyond current open-loop approaches and offering practical improvements in dynamic environments.</div>
    

    <div class="abstract">Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.MALLVi present a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.16898v2" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.16898v2
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">23</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.16802v1" target="_blank" rel="noopener">References Improve LLM Alignment in Non-Verifiable Domains</a>
        <div class="paper-meta">
          Kejian Shi, Yixin Liu, Peifeng Wang et al.
          &bull; 2026-02-18
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper addresses a significant challenge in LLM alignment – applying RLHF to non-verifiable domains – with a novel and effective approach using reference-guided LLM evaluators, demonstrating strong results comparable to existing methods and offering broader implications for self-improvement techniques.</div>
    

    <div class="abstract">While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft &#34;verifiers&#34;. First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.16802v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.16802v1
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">24</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.16708v2" target="_blank" rel="noopener">Policy Compiler for Secure Agentic Systems</a>
        <div class="paper-meta">
          Nils Palumbo, Sarthak Choudhary, Jihye Choi et al.
          &bull; 2026-02-18
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">This paper addresses a critical security gap in LLM-based agentic systems with a novel approach to deterministic policy enforcement using dependency graphs and a Datalog-derived language, offering significant implications for real-world deployments.</div>
    

    <div class="abstract">LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.
  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.
  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.16708v2" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.16708v2
    </a>
  </div>
  
  
  <div class="paper">
    <div class="paper-header">
      <div class="rank">25</div>
      <div class="paper-title">
        <a href="https://arxiv.org/abs/2602.16671v1" target="_blank" rel="noopener">SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation</a>
        <div class="paper-meta">
          Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand
          &bull; 2026-02-18
        </div>
      </div>
    </div>

    <div class="scores">
      <span class="badge badge-final">Final: 11</span>
      <span class="badge badge-llm-high">
        Impact: 8/10
      </span>
      <span class="badge" style="color: var(--muted)">Keywords: 3</span>
      
    </div>

    
    <div class="llm-reason">SPARC addresses a significant challenge in software testing with a novel neuro-symbolic approach, demonstrating strong empirical results and offering broader implications for reliable LLM-based code generation.</div>
    

    <div class="abstract">Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.</div>

    <a class="arxiv-link" href="https://arxiv.org/abs/2602.16671v1" target="_blank" rel="noopener">
      arXiv: https://arxiv.org/abs/2602.16671v1
    </a>
  </div>
  

  <footer>
    Apollo &mdash; Biweekly CS/AI Research Digest &bull;
    Powered by <a href="https://github.com/zeroclaw-labs/zeroclaw">ZeroClaw</a> &amp;
    <a href="https://gemini.google.com/">Gemini</a>
  </footer>

</div>
</body>
</html>